{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. What text cleaning methods did you try? Which are the ones you included in the final code?\n",
    "tried:\n",
    "- string lowercase\n",
    "- regular expression to remove punctuations, emoji(reference attached)\n",
    "- string strip to remove leading and trailing whitespace\n",
    "- regular expression to remove non-breaking leading whitespace\n",
    "- remove stop-words\n",
    "- stemming\n",
    "- spelling correction\n",
    "\n",
    "- final code: For the non-breaking leading whitespace that I can't figure out if it's a 'real space' or not, so I removed it. As CountVectorizer has a parameter of stop_words, I utilized this parameter when use the model.\n",
    "\n",
    "b. What are the features you considered using? Which features did you use in the final code?\n",
    "- bag-of-words for counting the frequency of each text token.\n",
    "- CountVectorizer for cleaning the text deeper and specifing the word and n-grams\n",
    "- TF-IDF normalization for downscaling the less meaningful high-frequency words\n",
    "\n",
    "- bag-of-words and CountVectorizer\n",
    "- CountVectorizer performs higher accuracy when the parameter of ngram_range sets to (1,2) than when it's fixed as 2.\n",
    "\n",
    "- I'd say the attribute 'year' is a useful feature, as from the excel chart, the attacking comments increase rapidly from 2001 to 2006. The attribute, 'logged_in' also tells something. The number of attacks is about 2 times when it's true for 'logged_in' than false.\n",
    "\n",
    "c. How did you decide to use the ‘attack’ information from different annotators? Did you average\n",
    "them, or use a number threshold, or did you use some other method to use this information?\n",
    "- I'd take any value that's greater or equal to 0.25 from the mean of the four types of attack annotators. That is to say, as long as there's one true value in any of the attack annotator, then it'd be considered for the training data. \n",
    "\n",
    "- If not taking the mean value into account, or just extract specifically the true value of the 'attack' annotator, when modeling the data, the code will not run but an error of single class. \n",
    "\n",
    "- The 'attack' annotator should suffice. So, if it's true, then the comment is trained as an attack.\n",
    "\n",
    "d. What optimizations did you add in your code, if any?\n",
    "- Added class_weight parameter to LogisticRegression and improved the accuracy to 0.90\n",
    "\n",
    "e. What are the ML methods you tried out, and what were your best results with each method?\n",
    "Which was the best ML method you saw before tuning hyperparameters?\n",
    "- LogisticRegression, MultinomialNB, RandomForestClassifier\n",
    "\n",
    "|                | LogisticRegression | MultinomialNB   | RandomForestClassifier |\n",
    "| -------------- | ------------------ | --------------- | ---------------------- |\n",
    "| accuracy_score | 0.89               | 0.88            | 0.97                   |\n",
    "| precision      | False       0.90   | False      0.88 | False             0.96 |\n",
    "|                | True        0.88   | True       0.85 | True              0.98 |\n",
    "| recall         | False       0.98   | False      0.97 | False             1.00 |\n",
    "|                | True        0.61   | True       0.55 | True              0.86 | \n",
    "| f1-score       | False       0.94   | False      0.93 | False             0.98 |\n",
    "|                | True        0.72   | True       0.67 | True              0.92 |\n",
    "\n",
    "- As shown above, the RandomForestClassifier has the highest accuracy, though the recall value for False class of '1.00' seems skeptical.  \n",
    "\n",
    "f. What hyper-parameter tuning did you do, and by how many percentage points did your accuracy\n",
    "go up because of hyper-parameter tuning?\n",
    "- Based on the results from RandomForestClassifier, used GridSearchCV to narrow the range of values for each hyperparameter.\n",
    "- Before fitting the model, transformed the string to integer by utilizing OneHotEncoder. Otherwise, will get a ValueError of \"Could not convert string to float\".\n",
    "- From the gridsearch.best_params_ results, added bootstrap, max_features, min_samples_leaf, min_samples_split, and n_estimators with suggested values.\n",
    "- It's possibily the size of the data that I shrinked to 1000 since the error, \"MemoryError: Unable to allocate 63.5 GiB for an array with shape (92704, 91981) and data type float64\". The accuracy went down after applying the hyper-parameters tuning and it's down by 4%.\n",
    "- Also, by changing the size of the date to 1000, the accuracy is much lower than with larger dataset.\n",
    "\n",
    "g. What did you learn from the different metrics? Did you try cross-validation?\n",
    "- Tried the k-fold cross-validation and added the confusion matrix. As for the different metrics, the confusion matrix is useful to calculate precision and recall. Precision and recall metrics can give information about how accurate and how relevant the predictions are.\n",
    "\n",
    "h. What are your best final Result Metrics? What is the increase in accuracy compared to the\n",
    "strawman figure? Which model gave you this performance?\n",
    "- The best one would be the RandomForestClassifier metrics, where the accuracy is 0.97, and with an increase of 6% comparing with the strawman code.\n",
    "\n",
    "i. What is the most interesting thing you learned from doing the report?\n",
    "- Trying out different machine learning models and getting different metrics\n",
    "- The part of Tuning Hyper-parameters is with quite a lot tools and it's quite fun. It's very import to keep track of the process, such as what's the current stage, is it still in preprocessing encoding; what's the current type of the dataset; how to connect two stages. As a newbie, it's easier for me to split up each steps. \n",
    "\n",
    "j. What was the hardest thing to do?\n",
    "- It'd be modifying the dataset for training and testing, and what to apply for the model's X and y. These are always confusing to me."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
